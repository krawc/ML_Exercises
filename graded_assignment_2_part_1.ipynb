{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graded Lab Assignment 2: Evaluate classifiers (10 points)\n",
    " \n",
    "In this assignment you will optimize and compare the perfomance of a parametric (logistic regression) and non-parametric (k-nearest neighbours) classifier on the MNIST dataset.\n",
    "\n",
    "Publish your notebook (ipynb file) to your Machine Learning repository on Github ON TIME. We will check the last commit on the day of the deadline.  \n",
    "\n",
    "### Deadline Friday, November 17, 23:59.\n",
    "\n",
    "This notebook consists of three parts: design, implementation, results & analysis. \n",
    "We provide you with the design of the experiment and you have to implement it and analyse the results.\n",
    "\n",
    "### Criteria used for grading\n",
    "* Explain and analyse all results.\n",
    "* Make your notebook easy to read. When you are finished take your time to review it!\n",
    "* You do not want to repeat the same chunks of code multiply times. If your need to do so, write a function. \n",
    "* The implementation part of this assignment needs careful design before you start coding. You could start by writing pseudocode.\n",
    "* In this exercise the insights are important. Do not hide them somewhere in the comments in the implementation, but put them in the Analysis part\n",
    "* Take care that all the figures and tables are well labeled and numbered so that you can easily refer to them.\n",
    "* A plot should have a title and axes labels.\n",
    "* You may find that not everything is 100% specified in this assignment. That is correct! Like in real life you probably have to make some choices. Motivate your choices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading points distribution\n",
    "\n",
    "* Implementation 5 points\n",
    "* Results and analysis 5 points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Design of the experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You do not have to keep the order of this design and are allowed to alter it if you are confident.\n",
    "* Import all necessary modules. Try to use as much of the available functions as possible. \n",
    "* Use the provided train and test set of MNIST dataset.\n",
    "* Pre-process data eg. normalize/standardize, reformat, etc.           \n",
    "  Do whatever you think is necessary and motivate your choices.\n",
    "* (1) Train logistic regression and k-nn using default settings.\n",
    "* Use 10-fold cross validation for each classifier to optimize the performance for one parameter: \n",
    "    * consult the documentation on how cross validation works in sklearn (important functions:             cross_val_score(), GridSearchCV()).\n",
    "    * Optimize k for k-nn,\n",
    "    * for logistic regression focus on the regularization parameter,\n",
    "* (2) Train logistic regression and k-nn using optimized parameters.\n",
    "* Show performance on the cross-validation set for (1) and (2) for both classifiers: \n",
    "    * report the average cross validation error rates (alternatively, the average accuracies - it's up to you) and standard deviation,\n",
    "    * plot the average cross valildation errors (or accuracies) for different values of the parameter that you tuned. \n",
    "* Compare performance on the test set for two classifiers:\n",
    "    * produce the classification report for both classifiers, consisting of precision, recall, f1-score. Explain and analyse the results.\n",
    "    * print confusion matrix for both classifiers and compare whether they missclassify the same  classes. Explain and analyse the results.\n",
    "* Discuss your results.\n",
    "* BONUS: only continue with this part if you are confident that your implemention is complete \n",
    "    * tune more parameters of logistic regression\n",
    "    * add additional classifiers (NN, Naive Bayes, decision tree), \n",
    "    * analyse additional dataset (ex. Iris dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------LOGISTIC REGRESSION:------------------------\n",
      "\n",
      "Cross-Validation: means & stds for given c-values\n",
      "Mean: 0.95133 | Standard Deviation:(+/-0.043) | for {'C': 0.01}\n",
      "Mean: 0.95133 | Standard Deviation:(+/-0.039) | for {'C': 0.03}\n",
      "Mean: 0.95267 | Standard Deviation:(+/-0.036) | for {'C': 0.1}\n",
      "Mean: 0.94533 | Standard Deviation:(+/-0.038) | for {'C': 0.3}\n",
      "Mean: 0.94333 | Standard Deviation:(+/-0.039) | for {'C': 0.5}\n",
      "Mean: 0.94267 | Standard Deviation:(+/-0.036) | for {'C': 1.0}\n",
      "Mean: 0.94067 | Standard Deviation:(+/-0.039) | for {'C': 3.0}\n",
      "Mean: 0.94000 | Standard Deviation:(+/-0.038) | for {'C': 5.0}\n",
      "Mean: 0.93800 | Standard Deviation:(+/-0.043) | for {'C': 10.0}\n",
      "\n",
      "\n",
      "Best score: 0.1\n",
      "\n",
      "Classification report for the best c-parameter (test set):\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.96      0.98        27\n",
      "          1       0.79      0.84      0.81        31\n",
      "          2       1.00      1.00      1.00        27\n",
      "          3       0.86      0.63      0.73        30\n",
      "          4       0.97      0.91      0.94        33\n",
      "          5       0.91      0.97      0.94        30\n",
      "          6       1.00      1.00      1.00        30\n",
      "          7       0.90      0.90      0.90        30\n",
      "          8       0.68      0.93      0.79        28\n",
      "          9       0.89      0.81      0.85        31\n",
      "\n",
      "avg / total       0.90      0.89      0.89       297\n",
      "\n",
      "\n",
      "Confusion matrix for the best c-parameter (test set):\n",
      "[[26  0  0  0  1  0  0  0  0  0]\n",
      " [ 0 26  0  2  0  0  0  0  3  0]\n",
      " [ 0  0 27  0  0  0  0  0  0  0]\n",
      " [ 0  1  0 19  0  3  0  2  5  0]\n",
      " [ 0  0  0  0 30  0  0  0  0  3]\n",
      " [ 0  1  0  0  0 29  0  0  0  0]\n",
      " [ 0  0  0  0  0  0 30  0  0  0]\n",
      " [ 0  1  0  0  0  0  0 27  2  0]\n",
      " [ 0  2  0  0  0  0  0  0 26  0]\n",
      " [ 0  2  0  1  0  0  0  1  2 25]]\n",
      "------------------------DECISION TREE:------------------------\n",
      "\n",
      "Cross-Validation: means & stds for given split-values\n",
      "Mean: 0.82933 | Standard Deviation:(+/-0.106) | for {'min_samples_split': 2}\n",
      "Mean: 0.82733 | Standard Deviation:(+/-0.108) | for {'min_samples_split': 3}\n",
      "Mean: 0.82400 | Standard Deviation:(+/-0.107) | for {'min_samples_split': 4}\n",
      "Mean: 0.82000 | Standard Deviation:(+/-0.113) | for {'min_samples_split': 5}\n",
      "\n",
      "\n",
      "Best score: 2\n",
      "\n",
      "Classification report for the best split-parameter (test set):\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.93      0.96      0.95        27\n",
      "          1       0.72      0.58      0.64        31\n",
      "          2       0.78      0.78      0.78        27\n",
      "          3       0.65      0.57      0.61        30\n",
      "          4       0.73      0.91      0.81        33\n",
      "          5       0.86      0.80      0.83        30\n",
      "          6       0.96      0.83      0.89        30\n",
      "          7       0.69      0.80      0.74        30\n",
      "          8       0.55      0.57      0.56        28\n",
      "          9       0.78      0.81      0.79        31\n",
      "\n",
      "avg / total       0.76      0.76      0.76       297\n",
      "\n",
      "\n",
      "Confusion matrix for the best split-parameter (test set):\n",
      "[[26  0  0  0  1  0  0  0  0  0]\n",
      " [ 0 18  3  1  3  0  0  0  5  1]\n",
      " [ 1  0 21  2  1  0  0  1  1  0]\n",
      " [ 0  1  1 17  0  2  1  4  4  0]\n",
      " [ 0  0  0  0 30  0  0  2  0  1]\n",
      " [ 0  0  1  0  3 24  0  0  1  1]\n",
      " [ 0  1  0  1  2  0 25  0  1  0]\n",
      " [ 0  0  0  4  1  0  0 24  0  1]\n",
      " [ 1  3  1  0  0  2  0  2 16  3]\n",
      " [ 0  2  0  1  0  0  0  2  1 25]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPkAAACgCAYAAADOzsqvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116320358>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#### %pylab inline\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn import datasets # to load the dataset\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler # to normalize data (NN is very sensitive to this!)\n",
    "from sklearn.model_selection import train_test_split #to split in train and test set\n",
    "from sklearn.linear_model import LogisticRegression #logistic regression classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier # neural network classifier\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score #BONUS: to tune parameters using cross-validation\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# load mnist dataset and split in train and test set.\n",
    "digits = load_digits()\n",
    "X_train_mnist = reshape(digits.images[:1500],(1500,64))\n",
    "X_test_mnist = reshape(digits.images[1500:],(297,64))\n",
    "y_train_mnist = digits.target[:1500]\n",
    "y_test_mnist = digits.target[1500:]\n",
    "\n",
    "\n",
    "plt.figure(1)\n",
    "\n",
    "# your implementation here\n",
    "\n",
    "def logRegr():\n",
    "    \n",
    "    #prepare c-values for cross validation\n",
    "    cVal = [0.01, 0.03, 0.1, 0.3, 0.5, 1., 3., 5., 10.]\n",
    "    \n",
    "    print('------------------------LOGISTIC REGRESSION:------------------------')\n",
    "    print('');\n",
    "    print('Cross-Validation: means & stds for given c-values');\n",
    "    #define the classifier\n",
    "    cv_regClass = LogisticRegression()\n",
    "    #use GridSearchCV to generate the best c-value for a classifier, from the cVal array\n",
    "    cv_reg = GridSearchCV(estimator=cv_regClass, param_grid = dict(C=cVal), cv=10, scoring='accuracy')\n",
    "    #fit the data\n",
    "    cv_reg.fit(X_train_mnist, y_train_mnist)\n",
    "    #print the mean and s.dev. scores for every c-value for comparison\n",
    "    means_cv_reg = cv_reg.cv_results_['mean_test_score']\n",
    "    stds_cv_reg = cv_reg.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means_cv_reg, stds_cv_reg, cv_reg.cv_results_['params']):\n",
    "        print(\"Mean: %0.5f | Standard Deviation:(+/-%0.03f) | for %r\" % (mean, std * 2, params))\n",
    "    print()\n",
    "   \n",
    "    #plot the mean accuracies for logistic regression CV\n",
    "    ax1 = plt.subplot(\"211\")\n",
    "    ax1.set_title(\"Average results for logistic regression CV\")\n",
    "    ax1.set_ylabel('avg accuracy in CV')\n",
    "    ax1.set_xlabel('regularization parameter')\n",
    "    ax1.plot(cVal, means_cv_reg)\n",
    "    print()\n",
    "    \n",
    "    #get the best c-value from grid search\n",
    "    best = cv_reg.best_params_['C']\n",
    "    #define logistic regression using the best value\n",
    "    cv_regBest = LogisticRegression(C=best)\n",
    "    #fit the training data\n",
    "    cv_regBest.fit(X_train_mnist, y_train_mnist)\n",
    "    #get the estimates\n",
    "    predicted = cv_regBest.predict(X_test_mnist)\n",
    "    \n",
    "    print('Best score: {}'.format(best))\n",
    "    print()\n",
    "    print('Classification report for the best c-parameter (test set):')\n",
    "    print(classification_report(y_test_mnist, predicted))\n",
    "    print()\n",
    "    print('Confusion matrix for the best c-parameter (test set):')\n",
    "    print(metrics.confusion_matrix(y_test_mnist, predicted))\n",
    "    \n",
    "def kNearest():\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    XTransformTrain = scaler.fit_transform(X_train_mnist, y_train_mnist)\n",
    "    XTransformTest = scaler.fit_transform(X_test_mnist, y_test_mnist)\n",
    "    \n",
    "    #prepare k-values for cross validation\n",
    "    kArr = [1,2,3,4,5,6,7,8,9,10]\n",
    "    \n",
    "    print('------------------------K-NEAREST NEIGHBORS:------------------------')        \n",
    "    print('');\n",
    "    print('Cross-Validation: means & stds for given c-values');\n",
    "    #define the knn-classifier\n",
    "    cv_knClass = KNeighborsClassifier()\n",
    "    #perform the grid search for different k-values\n",
    "    cv_knn = GridSearchCV(estimator=cv_knClass, param_grid = dict(n_neighbors=kArr), cv=10, scoring='accuracy')\n",
    "    #fi the data\n",
    "    cv_knn.fit(XTransformTrain, y_train_mnist)\n",
    "    #get and print means and stds from the cv_knn object\n",
    "    means_cv_knn = cv_knn.cv_results_['mean_test_score']\n",
    "    stds_cv_knn = cv_knn.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means_cv_knn, stds_cv_knn, cv_knn.cv_results_['params']):\n",
    "        print(\"Mean: %0.5f | Standard Deviation:(+/-%0.03f) | for %r\" % (mean, std * 2, params))\n",
    "            \n",
    "    #plot the mean accuracies for k-nearest neighbor CV\n",
    "    ax2 = plt.subplot(\"212\")\n",
    "    ax2.set_title(\"Average results for k-nearest neighbor CV\")\n",
    "    ax2.set_ylabel('avg accuracy in CV')\n",
    "    ax2.set_xlabel('k-value')\n",
    "    ax2.plot(kArr, means_cv_knn)\n",
    "    print()\n",
    "    \n",
    "    #get the best parameter from the cv_knn object \n",
    "    best = cv_knn.best_params_['n_neighbors']\n",
    "    #use it to define the new knn classifier\n",
    "    cv_knBest = KNeighborsClassifier(n_neighbors=best)\n",
    "    #fit the data\n",
    "    cv_knBest.fit(XTransformTrain, y_train_mnist)\n",
    "    #get estimates\n",
    "    predicted = cv_knBest.predict(XTransformTest)\n",
    "    \n",
    "    print('Best score: {}'.format(best))\n",
    "    print()\n",
    "    print('Classification report for the best k-score (test set):')\n",
    "    print(classification_report(y_test_mnist, predicted))\n",
    "    print()\n",
    "    print('Confusion matrix for the best k-score (test set):')\n",
    "    print(metrics.confusion_matrix(y_test_mnist, predicted))\n",
    "    \n",
    "    \n",
    "def tree():\n",
    "    \n",
    "    #prepare c-values for cross validation\n",
    "    cVal = [2, 3, 4, 5]\n",
    "    \n",
    "    print('------------------------DECISION TREE:------------------------')\n",
    "    print('');\n",
    "    print('Cross-Validation: means & stds for given split-values');\n",
    "    #define the classifier\n",
    "    cv_treeClass = DecisionTreeClassifier()\n",
    "    #use GridSearchCV to generate the best c-value for a classifier, from the cVal array\n",
    "    cv_tree = GridSearchCV(estimator=cv_treeClass, param_grid = dict(min_samples_split=cVal), cv=10, scoring='accuracy')\n",
    "    #fit the data\n",
    "    cv_tree.fit(X_train_mnist, y_train_mnist)\n",
    "    #print the mean and s.dev. scores for every c-value for comparison\n",
    "    means_cv_tree = cv_tree.cv_results_['mean_test_score']\n",
    "    stds_cv_tree = cv_tree.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means_cv_tree, stds_cv_tree, cv_tree.cv_results_['params']):\n",
    "        print(\"Mean: %0.5f | Standard Deviation:(+/-%0.03f) | for %r\" % (mean, std * 2, params))\n",
    "    print()\n",
    "   \n",
    "    #plot the mean accuracies for logistic regression CV\n",
    "    ax1 = plt.subplot(\"221\")\n",
    "    ax1.set_title(\"Average results for decision tree CV\")\n",
    "    ax1.set_ylabel('avg accuracy in CV')\n",
    "    ax1.set_xlabel('split parameter')\n",
    "    ax1.plot(cVal, means_cv_tree)\n",
    "    print()\n",
    "    \n",
    "    #get the best c-value from grid search\n",
    "    best = cv_tree.best_params_['min_samples_split']\n",
    "    #define logistic regression using the best value\n",
    "    cv_treeBest = DecisionTreeClassifier(min_samples_split=best)\n",
    "    #fit the training data\n",
    "    cv_treeBest.fit(X_train_mnist, y_train_mnist)\n",
    "    #get the estimates\n",
    "    predicted = cv_treeBest.predict(X_test_mnist)\n",
    "    \n",
    "    print('Best score: {}'.format(best))\n",
    "    print()\n",
    "    print('Classification report for the best split-parameter (test set):')\n",
    "    print(classification_report(y_test_mnist, predicted))\n",
    "    print()\n",
    "    print('Confusion matrix for the best split-parameter (test set):')\n",
    "    print(metrics.confusion_matrix(y_test_mnist, predicted))\n",
    "\n",
    "logRegr()\n",
    "#kNearest()\n",
    "tree()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and analysis of the experiment\n",
    "\n",
    "### 1. The dataset\n",
    "The data I am working on is a MNIST dataset, which consists of 1500 digitized samples of handwritten digits from 0 to 9. Each digit is denoted as an array of 64 digits that translate to brightness of pixels. Bright pixels define the contour of a number - therefore, the brighter the pixel, the higher the chance that it is within the range of a given number.\n",
    "\n",
    "### 2. Classifiers\n",
    "\n",
    "#### 2.1 Logistic Regression\n",
    "\n",
    "The logistic regression classifier used in the exercise comes from the sklearn kit. The full documentation is available on the website: http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html.\n",
    "\n",
    "The function LogisticRegression() takes several parameters typical for this type of model estimation. The one that needs most attention is the regularization parameter. It is responsible for simplifying or specifying our model in order to avoid overfit or underfit. It is done by muptiplying further features in the model by a specified number so that they have a greater or smaller influence on the model, depending on whether we want to decrease bias or decrease variance. \n",
    "\n",
    "The sklearn function uses a C parameter which is an inverse of the usual lambda penalizer. This means that the smaller the number, the less we take into account certain features, and the less fit to the data we have. \n",
    "\n",
    "At the beginning of the analysis, I put an array of selected c - parameters through a for loop and picked the one that returned the best accuracy rate (calculated with the metrics.accuracy_score() function). The best c - rate turned out to be *0.03*. Later on, I proceeded to try and conduct a 10-fold cross validation on the training data, which would supposedly return a more robust and accurate prediction for the best parameter (details further).\n",
    "\n",
    "#### 2.2 k-Nearest Neighbor\n",
    "\n",
    "The k-nearest neighbor classifier also comes from the sklearn kit. Docs: http://scikit-learn.org/stable/modules/neighbors.html\n",
    "\n",
    "The function KNeighborsClassifier calculates the distances from nearest neighbors, for all elements in the data set. Then based on the data, it returns k neighbors that are nearest to the test value, along with their indexes and target values. Based on the target values of nearest neighbors, and supported by a majority vote and other techniques of resolving ties, the algorithm returns a most lkely target value. \n",
    "\n",
    "The crucial value here is the k - number of nearest neighbors. There is always an optimum k-value that predicts with the highest accuracy, too small k might take anomalies into account and return mistaken results, while a too high k-value may extent the scope of neighbor detection beyond a reasonably accurate prediction.\n",
    "\n",
    "Overall, the technique works with surprisingly high accuracy for the MNIST data set. Without tweaking the parameters (having k set by default as 0) returned accuracies of up to 92% on the test set.\n",
    "\n",
    "After cross-validation, the accuracy score might get even higher. \n",
    "\n",
    "#### 2.3 Decision Tree\n",
    "\n",
    "The decision tree classifier also comes from the sklearn kit. Docs: http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier\n",
    "\n",
    "The goal is to create a model that predicts the value of a target variable by learning very simple decision rules from the data.\n",
    "\n",
    "It is basically slicing the data set into multiple decision boundaries.\n",
    "\n",
    "The variable we can manipulate here is the minimum number of samples required to make a slice. It has to be greater than one (otherwise anomalies would sneak in) but there is also an optimal value to it.\n",
    "This can be done using cross-validation.\n",
    "\n",
    "\n",
    "### 3. Cross-Validation \n",
    "\n",
    "#### 3.1 Logistic Regression\n",
    "\n",
    "The cross validation for logistic regression was supposed to find the value of regularization parameter that would work most accurately for predicting output. Essentially, in cross-validation you iterate through k chunks of data that you make out of your training set, and then calculate prediction accuracies for each of them.\n",
    "\n",
    "For finding the optimal value I used the GridSearchCV function. The function generates the best parameter out of a pre-made dictionary of parameters, and then fits the given data into an applicable regression model.\n",
    "\n",
    "Here, the cross validation was 10-fold, meaning that the training data was separated into 10 parts, each of which had an accuracy value estimated. Then, averages were taken for every c-value from the dictionary.\n",
    "\n",
    "The best c-value turned out to be: 0.1. Further, this value will be used for training the applicable data set and estimating the model, which in turn will be tested with the test set using a confusion matrix and classification report. Since its mean accuracy was the highest, and the estimator was relatively consistent (lowest standard deviation out of all parameters)\n",
    "\n",
    "| Best score | Mean   | Standard Dev.\n",
    "|------|------|------|\n",
    "|   0.1  | 0.95267|+/-0.036\n",
    "\n",
    "\n",
    "#### 3.2 k-Nearest Neighbor\n",
    "\n",
    "The cross validation for k-nearest neighbor was supposed to find the number of nearest neighbors that would work most accurately for predicting output.\n",
    "\n",
    "For finding the optimal value I used the GridSearchCV function again. The cross validation was 10-fold. Average accuracies were taken for every k-value from the dictionary.\n",
    "\n",
    "The best k-value turned out to be: 3. Further, this value will be used for training the applicable data set and estimating the model, which in turn will be tested with the test set using a confusion matrix and classification report.\n",
    "\n",
    "| Best score | Mean   | Standard Dev.\n",
    "|------|------|------|\n",
    "|   3  | 0.96133 |+/-0.045\n",
    "\n",
    "#### 3.3 Decision Tree\n",
    "\n",
    "The cross validation for decision tree was supposed to find the minimum number of samples required for a split that would work most accurately for predicting output.\n",
    "\n",
    "The best split-value turned out to be: 2. Further, this value will be used for training the applicable data set and estimating the model, which in turn will be tested with the test set using a confusion matrix and classification report.\n",
    "\n",
    "| Best score | Mean   | Standard Dev.\n",
    "|------|------|------|\n",
    "|   2  | 0.82933 |+/-0.106\n",
    "\n",
    "\n",
    "### 4. Performance & Results Analysis\n",
    "#### 3.1 Logistic Regression\n",
    "\n",
    "After running the logistic regression, the classification report outputted a precision value of 0.90.\n",
    "This means that in 90% of the cases, if a test sample was classified as a certain number, it was a match.\n",
    "Recall in this case is slightly lower - 0.89 here means that per the entire number of samples in each class, an average of 89% was classified correctly. This difference should not be of massive significance in the further process. The F1 estimator is a harmonic mean of precision and recall, calculated as follows: \n",
    "F1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "The best precision and recall has occurred for class of number 2 - all samples of 2s have been correctly assigned.\n",
    "An abnormally low value of precision can be noticed on class number 8 - only 0.68. This means that although most samples of 8s were correctly classified (high recall value), many numbers that weren't 8s were also classified as such. It could possibly be a few 3s, given their similar appearance, and the fact that for 3s the recall value is actually very low at 0.63.\n",
    "\n",
    "Surprisingly, relatively low values could be observed for number 1 - at around 0.8 both precision and recall.\n",
    "\n",
    "As for the *confusion matrix*, it is very apparent that the most misclassified class number was 3, with only 19 matches (should be 27). My hypothesis that it was missclassified as 8 got confirmed - this happened in 5 cases.\n",
    "1s were often also msitaken frequently for 7, 8 and 9.\n",
    "\n",
    "\n",
    "#### 3.2 k-Nearest Neighbor\n",
    "\n",
    "After running the k-nearest neighbor function, the classification report outputted an average precision AND recall value of 0.93. Interestingly, the rounded f1 value was lower than both precision and recall, which stems from the way harmonic mean is calculated per each class.\n",
    "\n",
    "Best values of precision and recall occurred for 0, where all samples were correctly assigned. Relatively high values also observable for 1. \n",
    "\n",
    "As for the *confusion matrix*, two digits can be considered under-classified: 2 and 3. For example, in 3 cases a 3 was classified as a 7. \n",
    "\n",
    "#### 3.3 Decision Tree\n",
    "\n",
    "After making a decision tree, the classification report outputted an average precision and recall value of 0.76. That's very low compared to other classifiers. It is also visible on the confusion matrix where we consistently have errors appearing on just about every number in more or less equal amounts.\n",
    "\n",
    "#### 3.3 Comparison\n",
    "\n",
    "Firstly, there is a marked difference in performance between decision trees and two other classifiers. DTs' precision and recall values are lower by up to 20%, therefore it is the least robust classifier out of the three tested, and can be put out of the question for further use - unless there are improvements that could be done in any other parameters.\n",
    "\n",
    "Overall, the k-nearest neighbor analysis has provided a greater precision and recall rate, as well as greater consistency (smaller STDs of these parameters) than logistic regression. \n",
    "\n",
    "Therefore, although both methods have limitations in recognizing different classes, K-Nearest Neighbor seems to be the more accurate, more consistent method for classifying handwriting samples from the MNIST dataset. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
